#!/usr/bin/env python
import argparse
import datetime
import pathlib
import sys

from bitcoinlib.keys import Key

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('-b', '--batch_size', required=False, default=1000, type=int,
                    help='Write batch size')
parser.add_argument('-r', '--read-size', required=False, default=4294967296, type=int,
                    help='Read batch size. Defaults to 4GB')


def get_p2pkh(k: Key):
    return k.address()


def get_p2wpkh(k: Key):
    return k.address(compressed=True, encoding="bech32")


def progress(p: float):
    p = int(p * 100)
    sys.stdout.write('\rProgress: {}%'.format(p))
    sys.stdout.flush()


def line_reader(file, chunk_size):
    remainder = ""  # Store the remaining data from the previous chunk
    while True:
        start = datetime.datetime.now()
        print(f"Reading batch of {chunk_size} bytes")
        chunk = file.read(chunk_size)
        if not chunk:
            print(f"EOF")
            break
        end = datetime.datetime.now()
        print(f"Batch read in {(end-start).seconds} seconds")
        data = remainder + chunk
        lines = data.split("\n")
        # Process all complete lines except the last one
        for line in lines[:-1]:
            yield line.strip()
        remainder = lines[-1]

    if remainder:
        yield remainder


if __name__ == "__main__":
    args = parser.parse_args()
    batch_size = args.batch_size
    path = pathlib.Path(args.input_path)
    hashes = set([])

    with open(path.joinpath("rel_address_address_header.csv"), "w+") as f:
        f.write("pk:START_ID(Address),address:END_ID(Address)\n")

    print("Starting..")
    chunk_size = args.read_size
    started_processing_pk = False
    with open(path.joinpath("addresses.csv"), "r") as f:
        # instead of writing every relationship one at a time, we write them in batches
        batch_write = []
        for line_number, line in enumerate(line_reader(f, chunk_size)):
            if line.startswith("pk_"):
                if line.endswith("CHECKSIG"):
                    pk = line[3:-12]
                else:
                    pk = line[3:]

                if not started_processing_pk:
                    started_processing_pk = True
                    print("Started processing public keys")

                try:
                    key = Key(pk)
                except Exception as e:
                    print(f"Failed to parse key, skipping\n{pk}\n{e}")
                    continue

                p2pkh = get_p2pkh(key)
                if p2pkh in hashes:
                    batch_write.append(f"{line},{p2pkh}")
                    continue

                p2wpkh = get_p2wpkh(key)
                if p2wpkh in hashes:
                    batch_write.append(f"{line},{p2wpkh}")
            else:
                hashes.add(line)

            if line_number and batch_write and line_number % batch_size == 0:
                date = datetime.datetime.utcnow()
                print(f'{date.strftime("%Y-%m-%d %H:%M:%S")} Processed {line_number} addresses')
                with open(path.joinpath("rel_address_address.csv"), "a") as output:
                    output.write("\n".join(batch_write) + "\n")
                batch_write = []

        if batch_write:
            with open(path.joinpath("rel_address_address.csv"), "a") as output:
                output.write("\n".join(batch_write) + "\n")
