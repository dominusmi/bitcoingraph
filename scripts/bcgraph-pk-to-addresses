#!/usr/bin/env python
import argparse
import pathlib
import sys

from bitcoinlib.keys import Key

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('-b', '--batch_size', required=False, default=1000, type=int,
                    help='Write batch size')
parser.add_argument('-r', '--read-size', required=False, default=4294967296, type=int,
                    help='Read batch size. Defaults to 4GB')


def get_p2pkh(k: Key):
    return k.address()


def get_p2wpkh(k: Key):
    return k.address(compressed=True, encoding="bech32")


def progress(p: float):
    p = int(p * 100)
    sys.stdout.write('\rProgress: {}%'.format(p))
    sys.stdout.flush()


def line_reader(file, chunk_size):
    remainder = b""  # Store the remaining data from the previous chunk
    while True:
        chunk = file.read(chunk_size)
        if not chunk:
            break

        data = remainder + chunk
        lines = data.split(b"\n")
        # Process all complete lines except the last one
        for line in lines[:-1]:
            yield line.decode()
        remainder = lines[-1]

    if remainder:
        yield remainder.decode()


if __name__ == "__main__":
    args = parser.parse_args()
    batch_size = args.batch_size
    path = pathlib.Path(args.input_path)
    hashes = set([])

    with open(path.joinpath("rel_address_address_header.csv"), "w+") as f:
        f.write("pk:START_ID(Address),address:END_ID(Address)\n")

    print("Starting..")
    line_count = 0
    count_hashes = 0
    count_pks = 0
    chunk_size = args.read_size
    with open(path.joinpath("addresses.csv"), 'rb') as file:
        for line in line_reader(file, chunk_size):
            line = line.strip()
            line_count += 1
            if line.startswith("pk_"):
                count_pks += 1

    count_hashes = line_count - count_pks
    print("Total PKs: {}\nTotal hashes: {}".format(count_pks, count_hashes))

    with open(path.joinpath("rel_address_address.csv"), "w+") as output:
        with open(path.joinpath("addresses.csv"), "rb") as f:
            # instead of writing every relationship one at a time, we write them in batches
            batch_write = []

            for line_number, line in enumerate(line_reader(f, chunk_size)):
                line = line.strip()
                if line.startswith("pk_"):
                    if line.endswith("CHECKSIG"):
                        pk = line[3:-12]
                    else:
                        pk = line[3:]

                    try:
                        key = Key(pk)
                    except Exception as e:
                        print(f"Failed to parse key, skipping\n{pk}\n{e}")
                        continue

                    p2pkh = get_p2pkh(key)
                    if p2pkh in hashes:
                        batch_write.append(f"{line},{p2pkh}")
                        continue

                    p2wpkh = get_p2wpkh(key)
                    if p2wpkh in hashes:
                        batch_write.append(f"{line},{p2wpkh}")
                else:
                    hashes.add(line)

                if line_number and batch_write and line_number % batch_size == 0:
                    progress(line_number / line_count)
                    output.write("\n".join(batch_write) + "\n")
                    batch_write = []

            if batch_write:
                output.write("\n".join(batch_write) + "\n")
