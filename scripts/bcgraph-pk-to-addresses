#!/usr/bin/env python

import argparse
import datetime
import pathlib
import sys
import os
import shutil
from bitcoinlib.keys import Key

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('-b', '--batch_size', required=False, default=1000, type=int,
                    help='Write batch size')
parser.add_argument('-r', '--read-size', required=False, default=4294967296, type=int,
                    help='Read batch size. Defaults to 4GB')


def get_p2pkh(k: Key):
    return k.address()


def get_p2wpkh(k: Key):
    return k.address(compressed=True, encoding="bech32")


def progress(p: float):
    p = int(p * 100)
    sys.stdout.write('\rProgress: {}%'.format(p))
    sys.stdout.flush()


def line_reader(file, chunk_size):
    remainder = ""  # Store the remaining data from the previous chunk
    while True:
        start = datetime.datetime.now()
        print(f"Reading batch of {chunk_size} bytes")
        chunk = file.read(chunk_size)
        if not chunk:
            print(f"EOF")
            break
        end = datetime.datetime.now()
        print(f"Batch read in {(end-start).seconds} seconds")
        data = remainder + chunk
        lines = data.split("\n")
        # Process all complete lines except the last one
        for line in lines[:-1]:
            yield line.strip()
        remainder = lines[-1]

    if remainder:
        yield remainder


if __name__ == "__main__":
    args = parser.parse_args()
    batch_size = args.batch_size
    path = pathlib.Path(args.input_path)

    with open(path.joinpath("rel_address_address_header.csv"), "w+") as f:
        f.write("pk:START_ID(Address),address:END_ID(Address)\n")

    print("Starting..")
    chunk_size = args.read_size
    started_processing_pk = False

    # Since we need to append to rel_address_address.csv within this run,
    # we must first remove any leftovers from a previous run.
    if os.path.isfile(path.joinpath("rel_address_address.csv")):
        os.remove(path.joinpath("rel_address_address.csv"))

    with open(path.joinpath("addresses.csv"), "r") as fa:
        # instead of writing every relationship one at a time, we write them in batches
        batch_write = []
        for line_number, line in enumerate(line_reader(fa, chunk_size)):
            if line.startswith("pk_"):
                if line.endswith("CHECKSIG"):
                    pk = line[3:-12]
                else:
                    pk = line[3:]

                if not started_processing_pk:
                    started_processing_pk = True
                    print("Started processing public keys")

                try:
                    key = Key(pk)
                except Exception as e:
                    print(f"Failed to parse key, skipping\n{pk}\n{e}")
                    continue

                p2pkh = get_p2pkh(key)
                batch_write.append(f"{line},{p2pkh}")

                p2wpkh = get_p2wpkh(key)
                batch_write.append(f"{line},{p2wpkh}")

            if line_number and batch_write and line_number % batch_size == 0:
                date = datetime.datetime.utcnow()
                print(f'{date.strftime("%Y-%m-%d %H:%M:%S")} Processed {line_number} addresses')
                with open(path.joinpath("rel_address_address.csv"), "a") as output:
                    output.write("\n".join(batch_write) + "\n")
                batch_write = []

        if batch_write:
            with open(path.joinpath("rel_address_address.csv"), "a") as output:
                output.write("\n".join(batch_write) + "\n")
    fa.close()

    pkh_addresses = []
    pk_lines = 0
    print("Reading pk2pkh addresses")
    with open(path.joinpath("rel_address_address.csv"), "r") as fr:
        for line_number, line in enumerate(line_reader(fr, chunk_size)):
            address = line.rsplit(",", 1)[-1] + "\n"
            pkh_addresses.append(address)
            pk_lines += 1
    fr.close()

    print("Writing pk2pkh addresses")
    with open(path.joinpath("pk2addresses.csv"), "w") as pka:
        pka.writelines(pkh_addresses)
    pka.close()

    print("Appending pk2pkh addresses to addresses")
    addrfiles = [path.joinpath("addresses.csv"), path.joinpath("pk2addresses.csv")]
    with open(path.joinpath("new_addresses.csv"), "wb") as ba:
        for af in addrfiles:
            with open(af,'rb') as fd:
                shutil.copyfileobj(fd, ba)

    # Sanity check and completion
    # print("pklines: " + str(pklines)  + " addresslines: " + str(addresses))
    # if not addresses.csv lines + pk2addresses.csv lines == new_addresses.csv lines:
    #    break
    # else
    #    sort unique new_addresses.csv
    #    overwrite addresses.csv with new_addresses.csv
    #    print completion message and a copy/paste neo4j CSV import command.

    # Do all this manually for now.