#!/usr/bin/env python

import argparse
import dataclasses
import logging
import pathlib
import pickle
import time
import uuid
from datetime import datetime
from queue import Queue
from threading import Thread
from typing import List, Set, Dict

import treelib.exceptions
from neo4j import GraphDatabase
from treelib import Tree

RUN_ID = uuid.uuid4()
logger: logging.Logger = None
transaction_logger: logging.Logger = None

def setup_logging():
    global RUN_ID, logger, transaction_logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s',
    )
    logger = logging.getLogger("entities")
    logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(f"track-bitcoin-output-{datetime.utcnow().isoformat()}-{RUN_ID}.log")
    logger.addHandler(file_handler)

    transaction_logger = logging.getLogger("transactions")
    file_handler = logging.FileHandler(f"track-bitcoin-transactions-{RUN_ID}.log")
    transaction_logger.addHandler(file_handler)
    transaction_logger.propagate = False


# @profile
def run_query(session, txid, result_queue, query):
    # Replace the following query with your specific query, using txid as a parameter
    result = session.run(query, txid=txid)
    result_queue.put((txid, result.values()))
    session.close()
def wrapper(idx, f, input_queue_: Queue, queue, session):
    while True:
        input = input_queue_.get()
        if input is None:
            break
        else:
            ret = f(session, input)
            queue.put((idx, ret))

# @profile
def parallel_execution(driver, thread_function, iterable, main_queue: Queue):


    threads = []
    result_queues = [Queue() for _ in range(20)]
    sessions = [driver.session() for i in range(20)]
    input_queues = [Queue() for _ in range(20)]
    try:
        for i in range(20):
            thread = Thread(target=wrapper, args=(i, thread_function, input_queues[i], result_queues[i], sessions[i]))
            thread.start()
            threads.append(thread)


        for i, elm in enumerate(iterable):
            sent = False
            if i < 20:
                input_queues[i].put(elm)
                continue

            while True:
                for queue in result_queues:
                    if not queue.empty():
                        idx, result = queue.get()
                        input_queues[idx].put(elm)
                        main_queue.put(result)
                        # yield result
                        sent=True
                        break
                if sent:
                    break
                time.sleep(0.1)

        for queue in input_queues:
            queue.put(None)

        # Wait for all threads to finish
        for thread in threads:
            thread.join()

        # Yield the results from the queue
        for result_queue in result_queues:
            while not result_queue.empty():
                rows = result_queue.get()[1]
                main_queue.put(rows)
                # yield rows

    finally:
        # try to make as clean an exit as possible
        main_queue.put(None)
        for queue in input_queues:
            queue.put(None)
        [session.close() for session in sessions]

class Walker(Tree):
    def __init__(self):
        super().__init__()
        self.create_node(identifier="root")

    def add_root(self, output: str):
        try:
            self.create_node(identifier=output, parent="root")
        except treelib.exceptions.DuplicatedNodeIdError:
            pass

    def get_path(self, node_id):
        path = []
        node = self.get_node(node_id)
        if node is None:
            return None  # Node not found
        while node is not None:
            path.append(node.identifier)
            node = self.parent(node.identifier)
        return list(reversed(path))

    def expand_node(self, txid: str, new_output: str, dollar_amount, address):
        if self.get_node(new_output) is None:
            self.create_node(identifier=new_output, parent=txid, data={"$": dollar_amount, "address": address})
            return True
        else:
            return False

    def remove_path(self, txid):
        try:
            parent = self.parent(txid)
        except treelib.exceptions.NodeIDAbsentError:
            return 0
        if len(self.children(parent.identifier)) == 1 and parent.identifier != "root":
            return self.remove_path(parent.identifier)
        else:
            deleted = self.remove_node(txid)
            return deleted

class InputWalker(Walker):
    def get_next_input(self):
        # we don't iterate of leaves directly since it can change as the loop goes on. This
        # way we get a snapshot
        leaves = list(self.leaves())
        for node in leaves:
            if "_" not in node.identifier:
                yield node.identifier

    def expand_transactions(self):
        leaves = list(self.leaves())
        for leaf in leaves:
            txid = leaf.identifier.split("_")[0]
            if self.get_node(txid) is None:
                self.create_node(identifier=txid, parent=leaf.identifier)
            else:
                self.remove_node(leaf.identifier)

class Summary:
    def __init__(self):
        self.expanded_transactions = 0
        self.depth = 1


MIN_AMOUNT: int = None
def thread_function(session, txid):
    global MIN_AMOUNT
    logger.debug(f"Running on {txid}")
    query_template = """
    MATCH (a:Address)<-[:USES]-(input:Output)-[:INPUT]->(t:Transaction {txid: $txid})<-[:CONTAINS]-(b:Block)
    WHERE input.value  > $min_value
    WITH collect([input.txid_n, input.value, a.address]) as tuple, t,b
    WITH t.txid as txid,b.timestamp as ts,tuple
    RETURN txid, ts, tuple
    """
    try:
        ts = session.run("MATCH (t:Transaction {txid: $txid})<-[:CONTAINS]-(b:Block) RETURN b.timestamp as ts", txid=txid).single().get("ts")
    except AttributeError:
        logger.warning(f"How can {txid} not have a block?!")
        return None
    btc_price = get_average_price(ts)
    result = session.run(query_template, txid=txid, min_value=MIN_AMOUNT/btc_price)
    return txid, result.values()

@dataclasses.dataclass
class Context:
    input_walker: InputWalker
    # transactions in current batch
    transactions: List[str]
    # transactions in current batch that were already processed
    done_transactions: Set[str]
    previous_transactions: Set[str]
    start: str
    end: str
    end_addresses: Set[str]
    address_to_outputs: Dict[str, List[str]]
    curr_depth: int
    max_depth: int
    min_amount: int
    ignore_entities: bool
    log_transactions: bool
    # When the context is created, this should be set to False. Only at the moment of saving should it be temporarily
    # set to True. This way, when we resume, we can correctly interpret the data
    resumed: bool


def get_outputs_through_entity(session, addresses: List[str]) -> List[str]:
    query_through_entity = """
    MATCH (a:Address)<-[:OWNER_OF]-()-[r:OWNER_OF]->()
    WHERE a.address in $addresses
    WITH a, count(r) as cr
    WHERE cr < 1000
    WITH a
    MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(b:Address)<-[r:USES]-()
    WHERE a <> b 
    WITH a, count(r) as cr
    WHERE cr < 1000
    MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(b:Address)<-[:USES]-(o:Output)
    RETURN collect(distinct o.txid_n) as txid_n
    """

    result = session.run(query_through_entity, addresses=list(addresses)).values()
    outputs = set([])
    if result and result[0] and result[0][0]:
        for o in result[0][0]:
            outputs.add(o)
    return list(outputs)

def track_output(driver, context: Context):
    global MIN_AMOUNT
    MIN_AMOUNT=context.min_amount

    logger.info(f"Start walk. Run id: {RUN_ID}")
    # with driver.session() as session:
    summary = Summary()
    last_save = time.time()
    to_remove = set()
    previous_transactions = context.previous_transactions

    try:
        for depth in range(context.curr_depth,context.max_depth):
            # for txid in walker.get_next_input():
                # for txid in walker.get_next_input():
            added_through_entity = 0
            if not context.ignore_entities and not context.resumed:
                with driver.session() as session:

                    addresses = {l.data["address"] for l in context.input_walker.leaves() if l.data is not None}
                    if addresses:
                        logger.debug(f"Getting outputs through entity for {addresses}")
                        outputs = get_outputs_through_entity(session, list(addresses))
                        for o in outputs:
                            added_through_entity += 1
                            context.input_walker.add_root(o)

            if context.resumed:
                transactions = list(set(context.transactions).difference(context.done_transactions))
            else:
                context.input_walker.expand_transactions()
                to_remove = set([])
                transactions = list(set(context.input_walker.get_next_input()))

                if context.log_transactions:
                    transaction_logger.info(f"Transactions expanded: {transactions}")

                logger.info(f"Total transactions expanded up to now: {summary.expanded_transactions}. "
                            f"Expanding {len(transactions)} transactions. Depth: {depth}")

                logger.debug(f"TX in common: {len(previous_transactions.intersection(set(transactions)))}")
                previous_transactions = set(transactions)

            receive_queue = Queue()
            thread = Thread(target=parallel_execution, args=(driver, thread_function, transactions, receive_queue))
            thread.start()

            # All the resume setup should be done by now, so we're effectively back into "normal" processing
            context.resumed = False

            done_transactions = set()
            while True:
                x = receive_queue.get()

                if x is None:
                    # this only happens when the thread as finished processing all transactions and it's the only
                    # way out of the while True loop (except for successfully finding a complete path)
                    break

                txid, rows = x
                if context.log_transactions:
                    transaction_logger.info(f"Explored {txid}")

                done_transactions.add(txid)
                summary.expanded_transactions += 1
                # rows = session.run(query_template, txid=txid).values()
                if not rows:
                    deleted = context.input_walker.remove_path(txid)
                    continue
                row = rows[0]
                ts = row[1]
                price = get_average_price(ts)
                for (txid_n, value, address) in row[2]:
                    if address in context.end_addresses:
                        logger.info("Found destination")
                        context.input_walker.expand_node(txid, txid_n, value*price, address)
                        logger.info(context.input_walker.get_path(txid_n))
                        return
                    if not context.input_walker.expand_node(txid, txid_n, value*price, address):
                        # implies the path is already checked and is therefore dead
                        to_remove.add(txid)

                # save progress every 6 hours
                if time.time() - last_save > 60*60*6:
                    logger.info("Saving progress")
                    context.resumed = True

                    # we need to save the current state to avoid re-running all the transactions at current depth
                    context.previous_transactions = previous_transactions
                    context.transactions = transactions
                    context.done_transactions = done_transactions
                    context.curr_depth = depth

                    with open(f"progress_{RUN_ID}.pickle", "wb+") as f:
                        pickle.dump(context, f)
                    context.resumed = False
                    last_save = time.time()

            if to_remove:
                for txid in to_remove:
                    node = context.input_walker.get_node(txid)
                    if node and len(context.input_walker.children(txid)) == 0:
                        logger.debug(f"Removing {txid}")
                        context.input_walker.remove_path(txid)

            summary.depth = depth
            context.curr_depth = depth
    finally:
        logger.info(f"Exited. Expanded a total of {summary.expanded_transactions} transactions up to depth {summary.depth}")


def resume_tracking(driver, path: str):
    global RUN_ID
    with open(pathlib.Path(path).resolve(), "rb") as f:
        ctx: Context = pickle.load(f)

    RUN_ID = path.split("_")[1].split(".")[0].strip()
    setup_logging()
    logger.info(f"Resuming from {RUN_ID}:\nMin amount: {ctx.min_amount}\nDepth: {ctx.curr_depth}->{ctx.max_depth}\n"
                f"Log transactions: {ctx.log_transactions}\nIgnore entities: {ctx.ignore_entities}")
    track_output(driver, ctx)

def setup_track_output(driver, start, end, min_amount, max_depth, ignore_entities, log_transactions):
    global MIN_AMOUNT
    MIN_AMOUNT = min_amount
    setup_logging()
    logger.info(f"Finding bitcoin trail {end}->{start}. Minimum transaction amount: {min_amount}$")
    if ignore_entities:
        logger.info("Ignoring paths through entities")

    address_to_outputs = {}
    walker = InputWalker()
    with driver.session() as session:
        logger.info(f"Getting addresses of entity {start}")
        result = session.run("""
                MATCH (e:Entity {name: $entityName})--(a:Address)--(o:Output)
                WITH a.address as address, collect(o.txid_n) as o
                RETURN address, o
            """, entityName=start).values()

        for address, outputs in result:
            address_to_outputs[address] = []
            for o in outputs:
                address_to_outputs[address].append(o)
                walker.add_root(o)
        logger.info(f"Addresses found: {address_to_outputs.keys()}")
        logger.info(f"Getting addresses of entity {end}")
        try:
            int(end)
            name = "entity_id"
        except:
            name = "name"

        end_addresses = session.run("""
            MATCH (e:Entity {%s: $destination})--(a:Address)
            RETURN collect(distinct a.address) as addresses
            """ % name, destination=end).values("addresses")
        end_addresses = set(end_addresses[0][0])

        logger.info(f"Addresses found: {end_addresses}")

    previous_transactions = set([])
    ctx = Context(input_walker=walker, transactions=[], previous_transactions=set([]), log_transactions=log_transactions,
                  address_to_outputs=address_to_outputs, end_addresses=end_addresses, curr_depth=0, max_depth=max_depth,
                  ignore_entities=ignore_entities, resumed=False, done_transactions=set([]), end=end, min_amount=min_amount,
                  start=start)

    setup_logging()
    track_output(driver, ctx)

def get_average_price(timestamp):
    data = {
        "Aug 21": 47_130.4, "Jul 21": 41_553.7, "Jun 21": 35_026.9, "May 21": 37_298.6,
        "Apr 21": 57_720.3, "Mar 21": 58_763.7, "Feb 21": 45_164.0, "Jan 21": 33_108.1,
        "Dec 20": 28_949.4, "Nov 20": 19_698.1, "Oct 20": 13_797.3, "Sep 20": 10_776.1,
        "Aug 20": 11_644.2, "Jul 20": 11_333.4, "Jun 20": 9_135.4, "May 20": 9_454.8,
        "Apr 20": 8_629.0, "Mar 20": 6_412.5, "Feb 20": 8_543.7, "Jan 20": 9_349.1,
        "Dec 19": 7_196.4, "Nov 19": 7_546.6, "Oct 19": 9_152.6, "Sep 19": 8_284.3,
        "Aug 19": 9_594.4, "Jul 19": 10_082.0, "Jun 19": 10_818.6, "May 19": 8_558.3,
        "Apr 19": 5_320.8, "Mar 19": 4_102.3, "Feb 19": 3_816.6, "Jan 19": 3_437.2,
        "Dec 18": 3_709.4, "Nov 18": 4_039.7, "Oct 18": 6_365.9, "Sep 18": 6_635.2,
    }

    given_date = datetime.fromtimestamp(timestamp)
    date_format = "%b %y"
    closest_date = min(data.keys(), key=lambda x: abs(given_date - datetime.strptime(x, date_format)))
    return data[closest_date]

parser = argparse.ArgumentParser(
    description="This binary is used to find the shortest direct bitcoin trail from one entity to another. The entity "
                "Can be provided both as entity_id (NOT the neo4j hidden id, the value entity.id), or as a name. "
                "By direct trail, we mean it only follows the following two types of trail:"
                "(receiver)<-[:USES]<-(:Output)<-[:INPUT]<-..(:Transaction)..<-[:OUTPUT]-(:Output)-[:USES]->(owner)"
                "Additionally, it goes through entities, so for each Output, it looks at the associated address and the entity,"
                "and if there's one, it also follows all the input to the entity addresses\n"
                "The parameter --min-amount-dollars can further be used to filter out Outputs that have a value"
                "smaller than the min amount. It uses the lower bound of the BTC value for the month of the transaction\n\n"
                "Keep in mind it knows which to call based on whether the id can be converted to an integer, so if you provide "
                "a name which is actually a number, it will try to get the entity with that entity_id instead of name."
)
parser.add_argument('receiver', help="Entity name or id receiving the bitcoins", nargs="?")
parser.add_argument('owner', help="Entity name or id sending the bitcoins", nargs="?")
parser.add_argument('--resume-path', help='Progress file from which to resume', required=False)
parser.add_argument('--host', default="0.0.0.0", help='Neo4j host')
parser.add_argument('--port', default=7687, help='Neo4j port', type=int)
parser.add_argument('-u', '--username', required=True, help='Neo4j user')
parser.add_argument('-p', '--password', required=True, help='Neo4j password')
parser.add_argument('-d', '--max-depth', required=False, help='Max depth to expand (in transactions)', default=100, type=int)
parser.add_argument('-m', '--min-amount-dollars', required=False, default=1000, type=int,
                    help="Skip expanding outputs if the associated value is lower than this (in dollars).")
parser.add_argument("--ignore-entities", action="store_true",
                    help="Do not go through entities along the way.")
parser.add_argument("--log-transactions", action="store_true",
                    help="Log all transactions explored")


if __name__ == "__main__":
    args = parser.parse_args()
    if (args.receiver is None or args.owner is None) and args.resume_path is None:
        print("You must either pass receiver and owner, or a resume path")
        exit(1)

    driver = GraphDatabase.driver(f"bolt://{args.host}:{args.port}",
                                  auth=(args.username, args.password),
                                  connection_timeout=3600)

    if args.resume_path:
        resume_tracking(driver, args.resume_path)
    else:
        setup_track_output(driver, args.receiver, args.owner, args.min_amount_dollars, args.max_depth, args.ignore_entities,
                       args.log_transactions)

