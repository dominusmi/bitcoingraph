#!/usr/bin/env python
import argparse
import bisect
import datetime
import io
import os
import pickle
import subprocess
import time
import typing
from collections import OrderedDict
from pathlib import Path
from typing import List, Optional

from tqdm import tqdm

from bitcoingraph import entities
from bitcoingraph.entities import EntityGrouping
from bitcoingraph.helper import sort

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('--sort-input', action='store_true',
                    help='Sort the rel_input.csv. This should already be done in the export except if you used '
                         'non-default arguments')
parser.add_argument('--sort-output', action='store_true',
                    help='Sort all input files. This is necessary if '
                         'the transaction deduplication was skipped on export.')
parser.add_argument('--chunk-size', type=int, default=1024 * 100,
                    help='Size of chunk read at a time from file, also batch-size (to be improved) in bytes')
parser.add_argument('--read-size', type=int, default=102400,
                    help='Size of chunk read at a time from file, also batch-size (to be improved) in bytes')
parser.add_argument('--cached-batches', type=int, default=10000,
                    help='Number of batches to keep in memory. Compute estimated memory as chunk-size x this')
parser.add_argument('--tot', type=int, default=None,
                    help='Total number of transactions, for progress bar')
parser.add_argument('--resume', type=str, default=None,
                    help='Pickle file to resume processing from')


class Cache:
    cumsum_row: List[int]
    lookup_table: List[int]
    lookup_keys: List[str]
    lookup_cursor: int

    cached_batches: List[List[str]]
    cache_cursor: int
    cache_txid_start: OrderedDict[str, int]
    n_cached_batches: int
    chunk_size: int

    def __init__(self, n_cached_batches: int, chunk_size: int, path_to_output_address: Path):
        self.cumsum_row = [0]
        self.lookup_table: List[int] = [0] * n_cached_batches
        self.lookup_keys: List[str] = [""] * n_cached_batches
        self.lookup_cursor: int = 0

        self.cached_batches: List[List[str]] = [[] for _ in range(n_cached_batches)]
        self.cache_cursor = 0
        self.cache_txid_start: OrderedDict[str, int] = OrderedDict()
        self.n_cached_batches = n_cached_batches
        self.chunk_size = chunk_size
        self.file: typing.BinaryIO = open(path_to_output_address, "rb")

    def __del__(self):
        print("Freeing cache resources")
        self.file.close()

    def load_indexes(self, output_address_iterator):
        progress = tqdm(output_address_iterator, desc="Indexing output")
        for i, (batch, cursor) in enumerate(progress):
            self.load_batch(batch, cursor, only_index=True)
            if i % 100 == 0:
                mb = float(cursor) / 1e3
                progress.set_postfix({"KB read": "{:,.0f}".format(mb)})

    def get_lookup_index(self, txid_n: str) -> Optional[int]:
        left = 0
        right = self.lookup_cursor - 1
        result = -1  # Default result if no match is found

        # if it's greater, we always return None. If it happens to be in the same batch
        # as the current greatest batch, it will be caught in the main code
        if txid_n >= self.lookup_keys[right]:
            return None

        while left <= right:
            mid = (left + right) // 2

            if self.lookup_keys[mid] > txid_n:
                result = mid
                right = mid - 1
            else:
                left = mid + 1

        return result - 1

    def try_get_cached(self, txid_n: str, index_position: int) -> Optional[str]:
        cache_idx = self.cache_txid_start.get(self.lookup_keys[index_position])
        if cache_idx is not None:
            return self.find_cached_address(cache_idx, txid_n)
        return None

    def load_from_file(self, lookup_index: int, txid_n: str):
        tell = self.lookup_table[lookup_index]
        self.file.seek(tell)
        buffer = self.file.read(self.chunk_size)
        buffer += self.file.readline()
        decoded = buffer.decode()
        batch, cursor = process_chunk(decoded, self.chunk_size, tell)
        batch_idx = self.load_batch(batch, cursor)
        address = self.find_cached_address(batch_idx, txid_n)
        if address is None:
            raise IndexError("load_from_file should always find the address")
        return address

    def find_cached_address(self, cache_idx: int, txid_n: str) -> Optional[str]:
        import bisect
        l = len(self.cached_batches[cache_idx])
        idx = bisect.bisect([row[0] for row in self.cached_batches[cache_idx]], txid_n)
        x = self.cached_batches[cache_idx][idx - 1]
        if idx == l and x[0] != txid_n:
            return None

        if x[0] == txid_n:
            return x[1]

    # @profile
    def load_batch(self, batch: List[List[str]], tell: int, only_index: bool = False):
        txid_n = batch[0][0]

        # This means it's not already loaded
        if self.lookup_cursor == 0 or tell > self.lookup_table[self.lookup_cursor - 1]:
            if (self.lookup_cursor + 1) % self.n_cached_batches == 0:
                self.lookup_table.extend([0] * self.n_cached_batches)
                self.lookup_keys.extend([""] * self.n_cached_batches)

            self.lookup_table[self.lookup_cursor] = tell
            self.lookup_keys[self.lookup_cursor] = txid_n
            self.lookup_cursor += 1

        if not only_index:
            # latest batch are automatically loaded in cache
            self.cache_txid_start[txid_n] = self.cache_cursor
            if len(self.cache_txid_start) >= self.n_cached_batches:
                # keep cache of max size self.n_cached_batches
                self.cache_txid_start.popitem(last=False)

            self.cached_batches[self.cache_cursor] = batch
            batch_idx = self.cache_cursor
            self.cache_cursor = (self.cache_cursor + 1) % self.n_cached_batches
            return batch_idx


def process_line(line):
    line = line.strip()
    idx = line.find(",")
    return line[:idx], line[idx + 1:]


def process_chunk(buffer: str, chunk_size: int, cursor: int):
    # list comprehensions are known to be faster due to underlying
    # optimisation done by CPython (not sure about PyPy)
    return [process_line(line) for line in buffer.split("\n") if line], cursor


def read_csv_in_chunks(file, chunk_size: int, read_size: int) -> (List[List[str]], int):  # 10MB
    s = file.read(read_size)
    s += file.readline()
    # batch_str = io.StringIO(s)
    # batch = io.TextIOWrapper(batch_str, encoding='utf-8')
    batch = io.BytesIO(s)
    file_tell = 0
    while True:
        buffer = batch.read(chunk_size).decode()
        cursor = file_tell
        while buffer:
            buffer += batch.readline().decode()  # finish the current line

            yield process_chunk(buffer, chunk_size, cursor)
            cursor = batch.tell() + file_tell
            buffer = batch.read(chunk_size).decode()

        file_tell = file.tell()
        tmp = file.read(read_size) + file.readline()
        if len(tmp) == 0:
            break
        batch = io.BytesIO(tmp)


def iterate_grouped_outputs(batch):
    """
    The batch has structure [[tx1,o11], [tx1,o12], [tx2,o21], [tx3,o31]...]. This generator groups all
    the outputs by the transaction, and will yield for example
    [o11,o12],
    [o21],
    [o31]
    """
    current_tx = None
    current_group = []

    for item in batch:
        tx, o = item
        if tx != current_tx:
            if current_group:
                yield current_tx, current_group, False
            current_tx = tx
            current_group = []
        current_group.append(o)

    if current_group:
        yield current_tx, current_group, True


def resume_iterator(curr_tx, iterator):
    for tx, outputs in iterator:
        if tx == curr_tx:
            return outputs
    return None


def output_distance(out1, out2):
    return int(out1[:3], 16) - int(out2[:3], 16)


# @profile
def fill_address_group_for_outputs(outputs: List[str], address_group: typing.Set[str], output_address_iterator,
                                   cache: Cache,
                                   queued_output_groups: Optional[List[List[str]]], max_distance=10):
    if queued_output_groups is not None:
        # Should be set to None when we're processing queued items, which are already sorted and with the distance check
        # already done
        outputs.sort()

        start_position = cache.lookup_keys[cache.lookup_cursor - 1] if cache.lookup_cursor > 0 else '0'
        if output_distance(outputs[-1], start_position) > max_distance:
            insert_at = bisect.bisect_right([x[-1] for x in queued_output_groups], outputs[-1])
            queued_output_groups.insert(insert_at, outputs)
            return 0

    skipped = 0
    for txid_n in outputs:
        idx = cache.get_lookup_index(txid_n)
        if idx is not None:
            address = cache.try_get_cached(txid_n, idx)
            if address is None:
                try:
                    address = cache.load_from_file(idx, txid_n)
                except IndexError:
                    print("Skipped idx found:", txid_n)
                    skipped += 1
                    # Can happen when the address was invalid (I suppose due to
                    # human error in the transaction)
                    continue
            address_group.add(address)
        else:
            # try to get from latest cache
            if cache.lookup_cursor > 0:
                tried_lookup = cache.lookup_cursor - 1
                address = cache.try_get_cached(txid_n, tried_lookup)
                if address is not None:
                    address_group.add(address)
                    continue

            for batch, cursor in output_address_iterator:
                batch_idx = cache.load_batch(batch, cursor)
                if batch[0][0] <= txid_n <= batch[-1][0]:
                    address = cache.find_cached_address(batch_idx, txid_n)
                    if address is not None:
                        address_group.add(address)
                        break
                if txid_n < batch[0][0]:
                    # Last chance, we try to get it from file now
                    try:
                        idx = cache.get_lookup_index(txid_n)
                        if idx:
                            address = cache.load_from_file(idx, txid_n)
                            if address is not None:
                                address_group.add(address)
                    except IndexError:
                        # This can happen when the address is "Unknown" (mistake?)
                        print("Skipped idx not found:", txid_n)
                        skipped += 1
                        break
    return skipped


# @profile
def process(input_path: Path, chunk_size: int, cached_batches: int, tot: Optional[int], read_size: int,
            resume: Optional[str]):
    print("Preparing cache")
    cache = Cache(n_cached_batches=cached_batches, chunk_size=chunk_size,
                  path_to_output_address=input_path.joinpath('rel_output_address.csv'))
    print("Cache ready")

    entity_grouping = EntityGrouping()
    tx_with_group = []
    curr_tx = None
    max_queue_size = 1_000_000
    max_distance = 20
    previous_time = time.time()
    resumed = True
    if resume is not None:
        print("WARNING: Resuming is not implemented for this version")
        # resumed = False
        # with open(Path(resume).resolve(), "rb") as resume_file:
        #     state_data = pickle.load(resume_file)
        #     entity_grouping = state_data["grouping"]
        #     curr_tx = state_data["tx"]

    try:
        with open(input_path.joinpath('rel_input.csv'), 'rb') as f_rel_input, open(
                input_path.joinpath('rel_output_address.csv'), 'rb') as f_rel_output_address:

            iteration = 0
            address_group = set([])
            skipped = 0
            total_count_tx = 0

            # INDEXING
            # indexing_path = pathlib.Path().resolve().joinpath(f"indexing_{tot}_{chunk_size}.pickle")
            # if indexing_path.exists():
            #     print("Loading existing index file")
            #     with open(indexing_path, "rb") as f_pickle:
            #         data = pickle.load(f_pickle)
            #         cache.lookup_cursor = data["lookup_cursor"]
            #         cache.lookup_keys = data["lookup_keys"]
            #         cache.lookup_table = data["lookup_table"]
            # else:
            #     print("Indexing rel_output_address.csv")
            #     with open(input_path.joinpath('rel_output_address.csv'), 'rb') as tmp:
            #         output_address_iterator = read_csv_in_chunks(tmp, chunk_size, read_size)
            #         cache.load_indexes(output_address_iterator)
            #         with open(indexing_path,
            #                   "wb+") as f_pickle:
            #             pickle.dump({
            #                 "lookup_cursor": cache.lookup_cursor,
            #                 "lookup_keys": cache.lookup_keys,
            #                 "lookup_table": cache.lookup_table
            #             }, f_pickle)

            if tot is not None:
                bar = tqdm(total=tot, unit='tx', mininterval=1, smoothing=0.1)
            else:
                bar = tqdm(unit='tx', mininterval=1, smoothing=0.1)

            output_address_iterator = read_csv_in_chunks(f_rel_output_address, chunk_size, read_size)

            queued_output_groups = []

            print("Starting main loop")
            interchunk_transaction_group = None
            for input_batch, _ in read_csv_in_chunks(f_rel_input, chunk_size, read_size):
                count_tx = 0
                iterator = iterate_grouped_outputs(input_batch)

                # Resume from previous state if necessary
                if not resumed:
                    outputs = resume_iterator(curr_tx, iterator)
                    if outputs is not None:
                        # We need to process the outputs found for the tx to make sure we don't lose out on an
                        # entity that was in between the current input_batch and the following.
                        fill_address_group_for_outputs(outputs, address_group, output_address_iterator, cache,
                                                       queued_output_groups)

                        resumed = True
                        print(f"Resumed from {curr_tx}")
                    else:
                        iteration += 1
                        continue

                for tx, outputs, leftover in iterator:
                    # if interchunk_transaction_group is not None and leftover:
                    #     if tx == interchunk_transaction_group[0]:
                    #         interchunk_transaction_group[1].extend(outputs)
                    #         continue

                    if interchunk_transaction_group is not None:
                        # We need to do this because a transaction group may be
                        # split into multiple returned batches
                        if interchunk_transaction_group[0] != tx:
                            skipped += fill_address_group_for_outputs(interchunk_transaction_group[1], address_group,
                                                                      output_address_iterator, cache,
                                                                      queued_output_groups)
                            entity_grouping.update_from_address_group(list(address_group))
                            address_group = set([])
                            curr_tx = tx
                        else:
                            outputs.extend(interchunk_transaction_group[1])

                        interchunk_transaction_group = None

                    if leftover:
                        interchunk_transaction_group = (tx, outputs)
                        continue

                    if len(outputs) == 1:
                        count_tx += 1
                        continue

                    skipped += fill_address_group_for_outputs(outputs, address_group, output_address_iterator, cache,
                                                              queued_output_groups, max_distance=max_distance)
                    entity_grouping.update_from_address_group(list(address_group))
                    address_group = set([])
                    count_tx += 1

                to_remove_from_queue = []
                dynamic_distance = int(round((4096 - max_distance) / max_queue_size * len(queued_output_groups)))
                start_position = cache.lookup_keys[cache.lookup_cursor - 1] if cache.lookup_cursor > 0 else '0'
                n_queued = len(queued_output_groups)
                for out_idx, queued_output_group in enumerate(queued_output_groups):
                    if n_queued - len(to_remove_from_queue) < max_queue_size and \
                            output_distance(queued_output_group[-1], start_position) > dynamic_distance:
                        # we can break since they're sorted by largest, so all the following would be larger
                        break

                    # the "real" address group is still in use at this point, and we don't
                    # want to pollute it, so we create this temporary one
                    tmp_address_group = set([])
                    skipped += fill_address_group_for_outputs(queued_output_group, tmp_address_group,
                                                              output_address_iterator, cache, None)
                    entity_grouping.update_from_address_group(list(tmp_address_group))
                    to_remove_from_queue.append(out_idx)
                    count_tx += 1

                for idx in reversed(to_remove_from_queue):
                    # have to use reversed because suppose indexes are [1,2,3], once we remove item [1], the item [2,3]
                    # will become [1,2]. Instead, if we remove [3] first, the position of [1] and [2] don't change
                    queued_output_groups.pop(idx)

                # Statistics section
                total_count_tx += count_tx
                bar.update(count_tx)
                bar.set_postfix({
                    "entities": len(entity_grouping.entity_idx_to_addresses),
                    "joined": entity_grouping.counter_joined_entities,
                    "iteration": iteration,
                    "skipped": skipped,
                    "queue size": "{:.0f}".format(len(queued_output_groups)),
                    "max_index": cache.lookup_keys[cache.lookup_cursor - 1][:5],
                    "max_distance": dynamic_distance
                })

                now = time.time()
                if now - previous_time > 600:
                    print("Dumping state")
                    previous_time = time.time()
                    with open(f"compute_entities_{curr_tx}_{datetime.datetime.utcnow().isoformat()[:-7]}", "wb+") as f:
                        pickle.dump({
                            "grouping": entity_grouping,
                            "tx": curr_tx
                        }, f)
                iteration += 1

            if len(address_group) > 1:
                entity_grouping.update_from_address_group(list(address_group))
                # tx_with_group.append(curr_tx)

            bar.set_description("Emptying queue")
            for queued_output_group in queued_output_groups:
                tmp_address_group = set([])
                skipped += fill_address_group_for_outputs(queued_output_group, tmp_address_group,
                                                          output_address_iterator, cache, None)
                entity_grouping.update_from_address_group(list(tmp_address_group))
                total_count_tx += 1

        bar.set_postfix({
            "entities": len(entity_grouping.entity_idx_to_addresses),
            "joined": entity_grouping.counter_joined_entities,
            "iteration": iteration,
            "skipped": skipped,
            "total_tx": total_count_tx
        })
        with open(f"compute_entities_end_{datetime.datetime.utcnow().isoformat()[:-7]}", "wb+") as f:
            pickle.dump({
                "grouping": entity_grouping,
            }, f)
    finally:
        bar.set_description("Saving files")
        with open(input_path.joinpath("rel_entity_address_header.csv"), "w+") as header:
            header.writelines(["entity_id:START_ID(Entity),address:END_ID(Address)"])

        with open(input_path.joinpath("entity_header.csv"), "w+") as header:
            header.writelines(["entity_id:ID(Entity)"])

        with open(input_path.joinpath("entity.csv"), "w+") as entity_f:
            buffer = ""
            for entity_id in entity_grouping.entity_idx_to_addresses.keys():
                buffer += f"{entity_id}\n"
                if len(buffer) > 10_000_000:
                    entity_f.write(buffer)

            entity_f.write(buffer)

        with open(input_path.joinpath(f"rel_entity_address.csv"), "w+") as result_csv:
            buffer = ""
            for entity_id, addresses in entity_grouping.entity_idx_to_addresses.items():
                buffer += "".join([f"{entity_id},{a}\n" for a in addresses if a is not None])

                # write the buffer in chunks of 1MB
                if len(buffer) > 1_000_000:
                    result_csv.write(buffer)
                    buffer = ""

            result_csv.write(buffer)


if __name__ == "__main__":
    args = parser.parse_args()
    sort_input = args.sort_input
    input_path = args.input_path

    if args.sort_output:
        print("Sorting rel_output_address.csv")
        sort(input_path, 'rel_output_address.csv')

    tot = args.tot
    if args.tot is None:
        print("Getting upper bound approximate number of transactions for statistics display")
        tx_path = Path(input_path).resolve().joinpath("transactions.csv")
        tot = int(round(os.path.getsize(tx_path) / 70))
        print("Found {} transactions".format(tot))

    start = time.time_ns()
    process(Path(input_path).resolve(), args.chunk_size, args.cached_batches, tot, args.read_size, args.resume)
    end = time.time_ns()
    print((end - start) / 1e9)
