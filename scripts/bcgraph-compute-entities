#!/usr/bin/env python
import argparse
import datetime
import io
import pickle
import time
import typing
from collections import OrderedDict
from pathlib import Path
from typing import List, Optional

from tqdm import tqdm

from bitcoingraph import entities
from bitcoingraph.entities import EntityGrouping
from bitcoingraph.helper import sort

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('--skip-sort-input', action='store_true',
                    help='This should only be set if the process crashed after reaching the log "Compute entities"')
parser.add_argument('--sort-output', action='store_true',
                    help='Sort all input files. This is necessary if '
                         'the transaction deduplication was skipped on export.')
parser.add_argument('--chunk-size', type=int, default=1024 * 100,
                    help='Size of chunk read at a time from file, also batch-size (to be improved) in bytes')
parser.add_argument('--read-size', type=int, default=102400,
                    help='Size of chunk read at a time from file, also batch-size (to be improved) in bytes')
parser.add_argument('--cached-batches', type=int, default=10000,
                    help='Number of batches to keep in memory. Compute estimated memory as chunk-size x this')
parser.add_argument('--tot', type=int, default=None,
                    help='Total number of transactions, for progress bar')
parser.add_argument('--resume', type=str, default=None,
                    help='Pickle file to resume processing from')


class Cache:
    cumsum_row: List[int]
    lookup_table: List[int]
    lookup_keys: List[str]
    lookup_cursor: int

    cached_batches: List[List[str]]
    cache_cursor: int
    cache_txid_start: OrderedDict[str, int]
    n_cached_batches: int
    chunk_size: int

    def __init__(self, n_cached_batches: int, chunk_size: int, path_to_output_address: Path):
        self.cumsum_row = [0]
        self.lookup_table: List[int] = [0] * n_cached_batches
        self.lookup_keys: List[str] = [""] * n_cached_batches
        self.lookup_cursor: int = 0

        self.cached_batches: List[List[str]] = [[] for _ in range(n_cached_batches)]
        self.cache_cursor = 0
        self.cache_txid_start: OrderedDict[str, int] = OrderedDict()
        self.n_cached_batches = n_cached_batches
        self.chunk_size = chunk_size
        self.file: typing.BinaryIO = open(path_to_output_address, "rb")

    def __del__(self):
        print("Freeing cache resources")
        self.file.close()

    def get_lookup_index(self, txid_n: str) -> Optional[int]:
        left = 0
        right = self.lookup_cursor - 1
        result = -1  # Default result if no match is found

        # if it's greater, we always return None. If it happens to be in the same batch
        # as the current greatest batch, it will be caught in the main code
        if txid_n > self.lookup_keys[right]:
            return None

        while left <= right:
            mid = (left + right) // 2

            if self.lookup_keys[mid] > txid_n:
                result = mid
                right = mid - 1
            else:
                left = mid + 1

        return result - 1

    def try_get_cached(self, txid_n: str, index_position: int) -> Optional[str]:
        cache_idx = self.cache_txid_start.get(self.lookup_keys[index_position])
        if cache_idx is not None:
            return self.find_cached_address(cache_idx, txid_n)
        return None

    def load_from_file(self, lookup_index: int, txid_n: str):
        tell = self.lookup_table[lookup_index]
        self.file.seek(tell)
        buffer = self.file.read(self.chunk_size)
        buffer += self.file.readline()
        batch, cursor, cumsum = process_chunk(buffer.decode(), self.chunk_size, tell)
        batch_idx = self.load_batch(batch, cursor, cumsum)
        address = self.find_cached_address(batch_idx, txid_n)
        if address is None:
            raise IndexError("load_from_file should always find the address")

    def find_cached_address(self, cache_idx: int, txid_n: str) -> Optional[str]:
        for row in self.cached_batches[cache_idx]:
            if row[0] == txid_n:
                return row[1]

        return None

    def load_batch(self, batch: List[List[str]], tell: int, cumsum: List[int]):
        txid_n = batch[0][0]

        # This means it's not already loaded
        if tell in self.lookup_table and self.lookup_cursor > 0:
            pass
        else:
            if (self.lookup_cursor + 1) % self.n_cached_batches == 0:
                self.lookup_table.extend([0] * self.n_cached_batches)
                self.lookup_keys.extend([""] * self.n_cached_batches)

            self.lookup_table[self.lookup_cursor] = tell
            self.lookup_keys[self.lookup_cursor] = txid_n
            self.lookup_cursor += 1

        # latest batch are automatically loaded in cache
        self.cache_txid_start[txid_n] = self.cache_cursor
        if len(self.cache_txid_start) >= self.n_cached_batches:
            # keep cache of max size self.n_cached_batches
            self.cache_txid_start.popitem(last=False)

        self.cached_batches[self.cache_cursor] = batch
        batch_idx = self.cache_cursor
        self.cache_cursor = (self.cache_cursor + 1) % self.n_cached_batches
        return batch_idx


def process_chunk(buffer: str, chunk_size: int, cursor: int):
    max_elements = int(round(chunk_size / 100))
    batch: List[List[str]] = [["", ""] for _ in range(max_elements)]
    cumsum = [0] * max_elements

    virtual_io = io.StringIO(buffer)
    n = 0
    for i, line in enumerate(virtual_io.readlines()):
        cumsum[i] = cursor + virtual_io.tell()
        row = line.split(",")
        batch[i][0] = row[0].strip()
        batch[i][1] = row[1].strip()
        n += 1

    return batch[:n], cursor, cumsum[:n]


def read_csv_in_chunks(file, chunk_size: int, read_size: int) -> (List[List[str]], int):  # 10MB
    s = file.read(read_size)
    s += file.readline()
    # batch_str = io.StringIO(s)
    # batch = io.TextIOWrapper(batch_str, encoding='utf-8')
    batch = io.BytesIO(s)
    file_tell = 0
    while True:
        buffer = batch.read(chunk_size).decode()
        cursor = file_tell
        while buffer:
            buffer += batch.readline().decode()  # finish the current line

            yield process_chunk(buffer, chunk_size, cursor)
            cursor = batch.tell() + file_tell
            buffer = batch.read(chunk_size).decode()

        file_tell = file.tell()
        tmp = file.read(read_size) + file.readline()
        if len(tmp) == 0:
            break
        batch = io.BytesIO(tmp)


def iterate_grouped_outputs(batch):
    """
    The batch has structure [[tx1,o11], [tx1,o12], [tx2,o21], [tx3,o31]...]. This generator groups all
    the outputs by the transaction, and will yield for example
    [o11,o12],
    [o21],
    [o31]
    """
    current_tx = None
    current_group = []

    for item in batch:
        tx, o = item
        if tx != current_tx:
            if current_group:
                yield current_tx, current_group
            current_tx = tx
            current_group = []
        current_group.append(o)

    if current_group:
        yield current_tx, current_group


def resume_iterator(curr_tx, iterator):
    for tx, outputs in iterator:
        if tx == curr_tx:
            return outputs
    return None


def process_outputs(outputs, address_group: typing.Set[str], output_address_iterator, cache: Cache):
    skipped = 0
    for txid_n in outputs:
        idx = cache.get_lookup_index(txid_n)
        if idx is not None:
            address = cache.try_get_cached(txid_n, idx)
            if address is None:
                try:
                    address = cache.load_from_file(idx, txid_n)
                except IndexError:
                    skipped += 1
                    # Can happen when the address was invalid (I suppose due to
                    # human error in the transaction)
                    continue
            address_group.add(address)
        else:
            # try to get from latest cache
            if cache.lookup_cursor > 0:
                tried_lookup = cache.lookup_cursor - 1
                address = cache.try_get_cached(txid_n, tried_lookup)
                if address is not None:
                    address_group.add(address)
                    continue

            for batch, cursor, cumsum in output_address_iterator:
                batch_idx = cache.load_batch(batch, cursor, cumsum)
                if batch[0][0] <= txid_n <= batch[-1][0]:
                    address = cache.find_cached_address(batch_idx, txid_n)
                    address_group.add(address)
                    break
                if txid_n < batch[0][0]:
                    # This can happen when the address is "Unknown" (mistake?)
                    skipped += 1
                    break
    return skipped


def process(input_path: Path, chunk_size: int, cached_batches: int, tot: Optional[int], read_size: int,
            resume: Optional[str]):

    print("Preparing cache")
    cache = Cache(n_cached_batches=cached_batches, chunk_size=chunk_size, path_to_output_address=input_path.joinpath('rel_output_address.csv'))

    entity_grouping = EntityGrouping()
    tx_with_group = []
    curr_tx = None
    resumed = True
    if resume is not None:
        resumed = False
        with open(Path(resume).resolve(), "rb") as resume_file:
            state_data = pickle.load(resume_file)
            entity_grouping = state_data["grouping"]
            curr_tx = state_data["tx"]

    try:
        with open(input_path.joinpath('rel_input.csv'), 'rb') as f_rel_input, open(
                input_path.joinpath('rel_output_address.csv'), 'rb') as f_rel_output_address:

            output_address_iterator = read_csv_in_chunks(f_rel_output_address, chunk_size, read_size)

            if tot is not None:
                print(f"Total: {tot}")
                bar = tqdm(read_csv_in_chunks(f_rel_input, chunk_size, read_size), total=tot)
            else:
                bar = tqdm(read_csv_in_chunks(f_rel_input, chunk_size, read_size))

            iteration = 0
            address_group = set([])
            skipped = 0
            for input_batch, _, _ in bar:
                iterator = iterate_grouped_outputs(input_batch)

                # Resume from previous state if necessary
                if not resumed:
                    outputs = resume_iterator(curr_tx, iterator)
                    if outputs is not None:
                        # We need to process the outputs found for the tx to make sure we don't lose out on an
                        # entity that was in between the current input_batch and the following.
                        process_outputs(outputs, address_group, output_address_iterator, cache)

                        resumed = True
                        print(f"Resumed from {curr_tx}")
                    else:
                        iteration += 1
                        continue

                for tx, outputs in iterator:
                    # We need to do this because a transaction group may be
                    # split into multiple returned batches
                    if curr_tx != tx:
                        if len(address_group) > 1:
                            entity_grouping.update_from_address_group(list(address_group))
                            # tx_with_group.append(curr_tx)
                        address_group = set([])
                        curr_tx = tx

                    skipped += process_outputs(outputs, address_group, output_address_iterator, cache)

                bar.update(len(input_batch))
                bar.set_postfix({
                    "entities": len(entity_grouping.entity_idx_to_addresses),
                    "joined": entity_grouping.counter_joined_entities,
                    "iteration": iteration,
                    "skipped": skipped
                })

                if iteration and iteration % 5000 == 0:
                    print("Dumping state")
                    with open(f"compute_entities_{curr_tx}_{datetime.datetime.utcnow().isoformat()[:-7]}", "wb+") as f:
                        pickle.dump({
                            "grouping": entity_grouping,
                            "tx": curr_tx
                        }
                            , f)
                iteration += 1

            if len(address_group) > 1:
                entity_grouping.update_from_address_group(list(address_group))
                # tx_with_group.append(curr_tx)

    finally:
        with open(f"results_{datetime.datetime.utcnow()}.csv", "w+") as result_csv:
            buffer = ""
            for entity_id, addresses in entity_grouping.entity_idx_to_addresses.items():
                buffer += "".join([f"{entity_id},{a}\n" for a in addresses])

                # write the buffer in chunks of 1MB
                if len(buffer) > 1_000_000:
                    result_csv.write(buffer)
                    buffer = ""

            result_csv.write(buffer)


if __name__ == "__main__":
    args = parser.parse_args()
    sort_input = not args.skip_sort_input
    input_path = args.input_path

    if args.sort_output:
        print("Sorting rel_output_address.csv")
        sort(input_path, 'rel_output_address.csv')

    if sort_input:
        print("Sorting rel_input.csv")
        sort(input_path, 'rel_input.csv', '-k 2 -t ,')
        entities.calculate_input_addresses(input_path)
        print("Sorting input_addresses.csv")
        sort(input_path, 'input_addresses.csv')

    start = time.time_ns()
    process(Path(input_path).resolve(), args.chunk_size, args.cached_batches, args.tot, args.read_size, args.resume)
    end = time.time_ns()
    print((end - start) / 1e9)
