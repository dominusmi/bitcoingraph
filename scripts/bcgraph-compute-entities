#!/usr/bin/env python
import argparse
import datetime
import io
import json
import time
import typing
from collections import OrderedDict
from pathlib import Path
from typing import List, Optional

from tqdm import tqdm

from bitcoingraph import entities
from bitcoingraph.entities import EntityGrouping
from bitcoingraph.helper import sort

parser = argparse.ArgumentParser(
    description='Compute entities from exported block chain data')
parser.add_argument('-i', '--input_path', required=True,
                    help='Input path')
parser.add_argument('--skip-sort-input', action='store_true',
                    help='This should only be set if the process crashed after reaching the log "Compute entities"')
parser.add_argument('--sort-output', action='store_true',
                    help='Sort all input files. This is necessary if '
                         'the transaction deduplication was skipped on export.')
parser.add_argument('--chunk-size', type=int, default=1024 * 100,
                    help='Size of chunk read at a time from file, also batch-size (to be improved) in bytes')
parser.add_argument('--cached-batches', type=int, default=10000,
                    help='Number of batches to keep in memory. Compute estimated memory as chunk-size x this')
parser.add_argument('--tot', type=int, default=None,
                    help='Total number of transactions, for progress bar')


class Cache:
    def __init__(self, n_cached_batches: int):
        self.cumsum_row = [0]
        self.lookup_table: List[int] = [0] * max(n_cached_batches,10000)
        self.lookup_keys: List[str] = [None] * max(n_cached_batches,10000)
        self.lookup_cursor = 0

        self.cached_batches: List[List[str]] = [[] for _ in range(n_cached_batches)]
        self.cache_cursor = 0
        self.cache_txid_start: OrderedDict = OrderedDict()
        self.n_cached_batches = n_cached_batches

    def get_lookup_index(self, txid_n: str) -> Optional[int]:
        for i, key in enumerate(self.lookup_keys[:self.lookup_cursor]):
            if txid_n < key:
                return i - 1
        return None

    def try_get_cached(self, txid_n: str, index_position: int) -> Optional[str]:
        cache_idx = self.cache_txid_start.get(self.lookup_keys[index_position])
        if cache_idx is not None:
            try:
                return self.find_cached_address(cache_idx, txid_n)
            except IndexError:
                pass
        return None

    def load_from_file(self, lookup_index: int, f: typing.TextIO, chunk_size: int, txid_n: str):
        tell = self.lookup_table[lookup_index]
        f.seek(tell)
        buffer = f.read(chunk_size)
        buffer += f.readline()
        batch, cursor, cumsum = process_chunk(buffer, chunk_size, tell)
        batch_idx = self.load_batch(batch, cursor, cumsum)
        return self.find_cached_address(batch_idx, txid_n)

    def find_cached_address(self, cache_idx: int, txid_n: str) -> str:
        for row in self.cached_batches[cache_idx]:
            if row[0] == txid_n:
                return row[1]
        raise IndexError("Address not found even though should be there")

    def load_batch(self, batch: List[List[str]], tell: int, cumsum: List[int]):
        txid_n = batch[0][0]

        # This means it's not already loaded
        if tell in self.lookup_table and self.lookup_cursor > 0:
            pass
        else:
            if (self.lookup_cursor + 1) % self.n_cached_batches == 0:
                self.lookup_table.extend([None] * self.n_cached_batches)
                self.lookup_keys.extend([None] * self.n_cached_batches)

            self.lookup_table[self.lookup_cursor] = tell
            self.lookup_keys[self.lookup_cursor] = txid_n
            self.lookup_cursor += 1

        # latest batch are automatically loaded in cache
        self.cache_txid_start[txid_n] = self.cache_cursor
        if len(self.cache_txid_start) >= self.n_cached_batches:
            # keep cache of max size self.n_cached_batches
            self.cache_txid_start.popitem(last=False)

        self.cached_batches[self.cache_cursor] = batch
        batch_idx = self.cache_cursor
        self.cache_cursor = (self.cache_cursor + 1) % self.n_cached_batches
        return batch_idx


def process_chunk(buffer: str, chunk_size: int, cursor: int):
    max_elements = int(round(chunk_size / 100))
    batch: List[List[str]] = [["", ""] for _ in range(max_elements)]
    cumsum = [0] * max_elements

    virtual_io = io.StringIO(buffer)
    n = 0
    for i, line in enumerate(virtual_io.readlines()):
        cumsum[i] = cursor + virtual_io.tell()
        row = line.split(",")
        batch[i][0] = row[0].strip()
        batch[i][1] = row[1].strip()
        n += 1

    return batch[:n], cursor, cumsum[:n]


def read_csv_in_chunks(file: typing.TextIO, chunk_size: int) -> (List[List[str]], int):  # 10MB
    buffer = file.read(chunk_size)
    cursor = 0
    while buffer:
        buffer += file.readline()  # finish the current line

        yield process_chunk(buffer, chunk_size, cursor)
        cursor = file.tell()
        buffer = file.read(chunk_size)


def iterate_grouped_outputs(batch):
    """
    The batch has structure [[tx1,o11], [tx1,o12], [tx2,o21], [tx3,o31]...]. This generator groups all
    the outputs by the transaction, and will yield for example
    [o11,o12],
    [o21],
    [o31]
    """
    current_tx = None
    current_group = []

    for item in batch:
        tx, o = item
        if tx != current_tx:
            if current_group:
                yield current_tx, current_group
            current_tx = tx
            current_group = []
        current_group.append(o)

    if current_group:
        yield current_tx, current_group


def process(input_path: Path, chunk_size: int, cached_batches: int, tot: Optional[int]):
    # chunk_size = 10*1024*1024 # 10MB
    print("Preparing cache")
    cache = Cache(n_cached_batches=cached_batches)

    entity_grouping = EntityGrouping()
    tx_with_group = []

    try:
        with open(input_path.joinpath('rel_input.csv'), 'r') as f_rel_input, open(
                input_path.joinpath('rel_output_address.csv'), 'r') as f_rel_output_address:

            output_address_iterator = read_csv_in_chunks(f_rel_output_address, chunk_size)

            if tot is not None:
                print(f"Total: {tot}")
                bar = tqdm(read_csv_in_chunks(f_rel_input, chunk_size), total=tot)
            else:
                bar = tqdm(read_csv_in_chunks(f_rel_input, chunk_size))

            curr_tx = None
            address_group = set([])
            for input_batch, _, _ in bar:
                for tx, outputs in iterate_grouped_outputs(input_batch):
                    # We need to do this because a transaction group may be
                    # split into multiple returned batches
                    if curr_tx != tx:
                        if len(address_group) > 1:
                            entity_grouping.update_from_address_group(list(address_group))
                            # tx_with_group.append(curr_tx)
                        address_group = set([])
                        curr_tx = tx

                    for txid_n in outputs:
                        idx = cache.get_lookup_index(txid_n)
                        if idx is not None:
                            address = cache.try_get_cached(txid_n, idx)
                            if address is None:
                                with open(input_path.joinpath('rel_output_address.csv'), 'r') as tmp:
                                    address = cache.load_from_file(idx, tmp, chunk_size, txid_n)
                            address_group.add(address)
                        else:
                            # try to get from latest cache
                            tried_lookup = None
                            if cache.lookup_cursor > 0:
                                tried_lookup = cache.lookup_cursor-1
                                address = cache.try_get_cached(txid_n, cache.lookup_cursor-1)
                                if address is not None:
                                    address_group.add(address)
                                    continue

                            for batch, cursor, cumsum in output_address_iterator:
                                batch_idx = cache.load_batch(batch, cursor, cumsum)
                                if batch[0][0] <= txid_n <= batch[-1][0]:
                                    address = cache.find_cached_address(batch_idx, txid_n)
                                    address_group.add(address)
                                    break
                                if txid_n < batch[0][0]:

                                    raise IndexError("\n".join([
                                        "Cannot happen. Summary:",
                                        f"Highest key: {cache.lookup_keys[cache.lookup_cursor-1]}",
                                        f"Looking for: {txid_n}",
                                        f"Batch min:   {batch[0][0]}",
                                        f"Tried to get {tried_lookup} key -> {cache.lookup_keys[tried_lookup]}",
                                        f""
                                    ]))

                bar.update(len(input_batch))
                bar.set_postfix({"entities": len(entity_grouping.entity_idx_to_addresses),
                                 "joined": entity_grouping.counter_joined_entities})

            if len(address_group) > 1:
                entity_grouping.update_from_address_group(list(address_group))
                # tx_with_group.append(curr_tx)

    finally:
        with open(f"results_{datetime.datetime.utcnow()}.json", "w+") as f:
            json.dump({key: list(value) for key, value in entity_grouping.entity_idx_to_addresses.items()}, f)


if __name__ == "__main__":
    args = parser.parse_args()
    sort_input = not args.skip_sort_input
    input_path = args.input_path

    if args.sort_output:
        print("Sorting rel_output_address.csv")
        sort(input_path, 'rel_output_address.csv')

    if sort_input:
        print("Sorting rel_input.csv")
        sort(input_path, 'rel_input.csv', '-k 2 -t ,')
        entities.calculate_input_addresses(input_path)
        print("Sorting input_addresses.csv")
        sort(input_path, 'input_addresses.csv')

    start = time.time_ns()
    process(Path(input_path).resolve(), args.chunk_size, args.cached_batches, args.tot)
    end = time.time_ns()
    print((end - start) / 1e9)
