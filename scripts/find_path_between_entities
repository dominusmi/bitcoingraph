#!/usr/bin/env python

import argparse
import dataclasses
import logging
import pathlib
import pickle
import time
import uuid
from datetime import datetime
from queue import Queue
from threading import Thread
from typing import List, Set, Dict, Tuple

import treelib.exceptions
from neo4j import GraphDatabase
from treelib import Tree

RUN_ID = uuid.uuid4()
logger: logging.Logger = None
transaction_logger: logging.Logger = None

def setup_logging():
    global RUN_ID, logger, transaction_logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s',
    )
    logger = logging.getLogger("entities")
    logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(f"track-bitcoin-output-{datetime.utcnow().isoformat()}-{RUN_ID}.log")
    logger.addHandler(file_handler)

    transaction_logger = logging.getLogger("transactions")
    file_handler = logging.FileHandler(f"track-bitcoin-transactions-{RUN_ID}.log")
    transaction_logger.addHandler(file_handler)
    transaction_logger.propagate = False


# @profile
def run_query(session, txid, result_queue, query):
    # Replace the following query with your specific query, using txid as a parameter
    result = session.run(query, txid=txid)
    result_queue.put((txid, result.values()))
    session.close()
def wrapper(idx, f, input_queue_: Queue, queue, session):
    while True:
        input = input_queue_.get()
        if input is None:
            break
        else:
            ret = f(session, input)
            queue.put((idx, ret))

# @profile
def parallel_execution(driver, thread_function, kill_queue, iterable, main_queue: Queue):


    threads = []
    result_queues = [Queue() for _ in range(20)]
    sessions = [driver.session() for i in range(20)]
    input_queues = [Queue() for _ in range(20)]
    try:
        for i in range(20):
            thread = Thread(target=wrapper, args=(i, thread_function, input_queues[i], result_queues[i], sessions[i]))
            thread.start()
            threads.append(thread)


        for i, elm in enumerate(iterable):
            if not kill_queue.empty():
                logger.info("Received thread kill signal. Stopping")
                break
            sent = False
            if i < 20:
                input_queues[i].put(elm)
                continue

            while True:
                for queue in result_queues:
                    if not queue.empty():
                        idx, result = queue.get()
                        input_queues[idx].put(elm)
                        main_queue.put(result)
                        # yield result
                        sent=True
                        break
                if sent:
                    break
                time.sleep(0.1)

        for queue in input_queues:
            queue.put(None)

        # Wait for all threads to finish
        for thread in threads:
            thread.join()

        # Yield the results from the queue
        for result_queue in result_queues:
            while not result_queue.empty():
                rows = result_queue.get()[1]
                main_queue.put(rows)
                # yield rows

    finally:
        # try to make as clean an exit as possible
        main_queue.put(None)
        for queue in input_queues:
            queue.put(None)
        [session.close() for session in sessions]

class Walker(Tree):
    garbage: Set[str]
    txids: Set[str]

    def __init__(self):
        super().__init__()
        self.garbage = set([])
        self.create_node(identifier="root")
        self.txids = set([])

    def add_root(self, output: str, data=None):
        try:
            self.create_node(identifier=output, parent="root", data=data)
        except treelib.exceptions.DuplicatedNodeIdError:
            pass

    def get_path(self, node_id):
        path = []
        node = self.get_node(node_id)
        if node is None:
            return None  # Node not found
        while node is not None:
            path.append(node.identifier)
            if node and node.data and node.data.get("entity"):
                address = node.data["address"]
                path.append(f"Entity[{address}]")

            node = self.parent(node.identifier)


        return list(reversed(path))


    def remove_path(self, identifier):
        node = self.get_node(identifier)
        if node is None:
            logger.warning(f"Trying to remove unknown identifier: {identifier}")
        else:
            if node.data:
                node.data["end"] = True
            else:
                node.data = {"end": True}
        # try:
        #     parent = self.parent(identifier)
        # except treelib.exceptions.NodeIDAbsentError:
        #     return 0
        # if len(self.children(parent.identifier)) == 1 and parent.identifier != "root":
        #     return self.remove_path(parent.identifier)
        # else:
        #     deleted = self.remove_node(identifier)
        #     return deleted


    def add_to_garbage(self, identifier: str):
        self.garbage.add(identifier)

    def empty_garbage(self):
        for identifier in self.garbage:
            node = self.get_node(identifier)
            if node and len(self.children(identifier)) == 0:
                logger.debug(f"Removing {identifier}")
                self.remove_path(identifier)

class InputWalker(Walker):
    def get_next_input(self):
        # we don't iterate of leaves directly since it can change as the loop goes on. This
        # way we get a snapshot
        leaves = list(self.leaves())
        for node in leaves:
            if node.data and node.data.get("end") == True:
                continue

            if "_" not in node.identifier:
                yield node.identifier

    def expand_node(self, txid: str, new_output: str, dollar_amount, address):
        if self.get_node(new_output) is None:
            self.create_node(identifier=new_output, parent=txid, data={"$": dollar_amount, "address": address})
            return True
        else:
            return False

    def expand_transactions(self):
        leaves = list(self.leaves())
        for leaf in leaves:
            txid = leaf.identifier.split("_")[0]

            if txid in self.txids:
                self.remove_path(leaf.identifier)
            else:
                self.txids.add(txid)
                self.create_node(identifier=txid, parent=leaf.identifier)

    def process_input(self, txid, rows, end_addresses):
        if not rows:
            self.remove_path(txid)
            return
        row = rows[0]
        ts = row[1]
        price = get_average_price(ts)
        for (txid_n, value, address) in row[2]:
            if address in end_addresses:
                logger.info("Found destination")
                self.expand_node(txid, txid_n, value * price, address)
                logger.info(self.get_path(txid_n))
                exit(0)
            if not self.expand_node(txid, txid_n, value * price, address):
                # implies the path is already checked and is therefore dead
                self.add_to_garbage(txid)

    def add_entity_connection(self, output: str, address: str):
        if self.get_node(output) is not None:
            return
        parent = next(self.filter_nodes(lambda x: x.data and x.data.get("address") == address and x.data.get("entity") != True))
        self.create_node(identifier=output, parent=parent, data={"entity": True, "address": address})

class OutputWalker(Walker):
    def add_entity_connection(self, rows):
        for row in rows:
            txid_n, address, new_txid = row
            if self.get_node(txid_n) is not None or self.get_node(new_txid) is not None:
                continue

            parent = next(self.filter_nodes(lambda x: x.data and x.data.get("address") == address and x.data.get("entity") != True))
            self.create_node(identifier=txid_n, parent=parent, data={"entity": True, "address": address})
            self.create_node(identifier=new_txid, parent=txid_n)

    def process_output(self, txid, rows, through_entity=False):
        if not rows and txid != 'root':
            self.remove_path(txid)
            return

        for row in rows:
            txid_n, address, new_txid = row
            node_txid_n = self.get_node(txid_n)
            if node_txid_n is None:
                data = {"address": address}
                if through_entity:
                    data["entity"] = True
                self.create_node(identifier=txid_n, parent=txid, data=data)

            if new_txid in self.txids:
                continue
            self.txids.add(new_txid)
            node_txid = self.get_node(new_txid)
            if node_txid is None:
                self.create_node(identifier=new_txid, parent=txid_n)
            else:
                self.remove_path(new_txid)

    def get_next_output(self):
        # we don't iterate of leaves directly since it can change as the loop goes on. This
        # way we get a snapshot
        leaves = list(self.leaves())
        for node in leaves:
            if node.data and node.data.get("end") == True:
                continue

            if "_" not in node.identifier:
                yield node.identifier

    def get_latest_addresses(self):
        addresses = set([])
        for leaf in self.leaves():
            if leaf.data is not None and leaf.data.get("end") == True:
                continue

            node = self.parent(leaf.identifier)
            if node.data is None or node.data.get("address") is None:
                continue

            addresses.add(node.data["address"])
        return addresses
class Summary:
    def __init__(self):
        self.expanded_transactions = 0
        self.depth = 1


def path_to_query(input_path, output_path):
    query = "MATCH path="
    previous = None

    path = input_path + list(reversed(output_path[1:-1]))

    len_input_path = len(input_path)
    for i,e in enumerate(path):
        if e == "root":
            query += "(:Entity)-[:OWNER_OF]->(:Address)"
            previous = "root"
        elif "_" in e:
            if previous == "root" or previous == "entity":
                query += "<-[:USES]-"
            elif previous == "tx":
                query += "<-[:INPUT]-"
            query += f"(:Output {{txid_n: '{e}'}})"
            previous = "output"

        elif "Entity" in e:
            address = e.split("[")[1].split("]")[0]
            # The only difference between the input and output direction, is whether the entity address
            # is to the left or right of the entity.
            if i < len_input_path:
                query += f"-[:USES]->(:Address {{ address: '{address}'}})<-[:OWNER_OF]-(:Entity)-[:OWNER_OF]->(:Address)"
            else:
                query += f"-[:USES]->(:Address)<-[:OWNER_OF]-(:Entity)-[:OWNER_OF]->(:Address {{ address: '{address}'}})"
            previous = "entity"

        else:
            query += f"<-[:OUTPUT]-(:Transaction {{ txid: '{e}' }})"
            previous = "tx"

    query += "-[:USES]->(:Address)<-[:OWNER_OF]-(:Entity) RETURN path"
    return query

MIN_AMOUNT: int = None
def input_thread_function(session, txid):
    global MIN_AMOUNT
    logger.debug(f"Running on {txid}")
    query_template = """
    MATCH (a:Address)<-[:USES]-(input:Output)-[:INPUT]->(t:Transaction {txid: $txid})<-[:CONTAINS]-(b:Block)
    WHERE input.value  > $min_value
    WITH collect([input.txid_n, input.value, a.address]) as tuple, t,b
    WITH t.txid as txid,b.timestamp as ts,tuple
    RETURN txid, ts, tuple
    """
    try:
        ts = session.run("MATCH (t:Transaction {txid: $txid})<-[:CONTAINS]-(b:Block) RETURN b.timestamp as ts", txid=txid).single().get("ts")
    except AttributeError:
        logger.warning(f"How can {txid} not have a block?!")
        return None
    btc_price = get_average_price(ts)
    result = session.run(query_template, txid=txid, min_value=MIN_AMOUNT/btc_price)
    return txid, True, result.values()

def output_thread_function(session, txid):
    global MIN_AMOUNT
    logger.debug(f"Running on {txid}")

    results = session.run(
        """MATCH (:Transaction {txid: $txid})-[:OUTPUT]->(output:Output)-[:INPUT]->(t:Transaction)<-[:CONTAINS]-(b:Block) 
        WITH output, b.timestamp as ts, t.txid as txid 
        MATCH (output)-[:USES]->(a:Address) 
        RETURN output.txid_n as txid_n, output.value as value, a.address as address, ts, txid
        """
        , txid=txid
    ).values()

    tuples = []
    for row in results:
        if get_average_price(row[3])*row[1] > MIN_AMOUNT:
            # txid_n, address, txid
           tuples.append((row[0], row[2], row[4]))
    return txid, False, tuples


@dataclasses.dataclass
class Context:
    input_walker: InputWalker
    output_walker: OutputWalker
    # transactions in current batch
    transactions: List[str]
    output_transactions: List[str]
    # transactions in current batch that were already processed
    done_transactions: Set[str]
    previous_transactions: Set[str]
    start: str
    end: str
    end_addresses: Set[str]
    address_to_outputs: Dict[str, List[str]]
    curr_depth: int
    max_depth: int
    min_amount: int
    ignore_entities: bool
    log_transactions: bool
    # When the context is created, this should be set to False. Only at the moment of saving should it be temporarily
    # set to True. This way, when we resume, we can correctly interpret the data
    resumed: bool


def get_inputs_through_entity(session, addresses: List[str]) -> Dict[str, List[str]]:
    query_through_entity = """
    MATCH (a:Address)<-[:OWNER_OF]-()-[r:OWNER_OF]->()
    WHERE a.address in $addresses
    WITH a, count(r) as cr
    WHERE cr < 1000
    CALL {
        WITH a
        MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(b:Address)<-[r:USES]-()
        WHERE a <> b 
        WITH a, count(r) as cr
        WHERE cr < 1000
        MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(b:Address)<-[:USES]-(o:Output)
        RETURN a.address as address, collect(distinct o.txid_n) as txid_n
    } IN TRANSACTIONS OF 100 ROWS
    RETURN address, txid_n
    """
    address_to_outputs = {}

    rows = session.run(query_through_entity, addresses=list(addresses)).values()

    if rows and rows[0] and rows[0][0]:
        for row in rows:
            address, txid_n = row
            address_to_outputs[address] = txid_n

    return address_to_outputs


def get_outputs_through_entity(session, addresses: List[str]) -> List[Tuple[str, str, str]]:
    query_through_entity = """
    MATCH (a:Address)<-[:OWNER_OF]-()-[r:OWNER_OF]->()
    WHERE a.address in $addresses
    WITH a, count(r) as cr
    WHERE cr < 1000
    CALL {
        WITH a
        MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(b:Address)<-[r:USES]-()
        WHERE a <> b 
        WITH a, count(r) as cr
        WHERE cr < 1000
        MATCH (a)<-[:OWNER_OF]-()-[:OWNER_OF]->(:Address)<-[:USES]-(o:Output)-[:INPUT]->(t:Transaction)<-[:CONTAINS]-(block:Block)
        RETURN o.txid_n as txid_n, a.address as address, t.txid as txid, o.value as value, block.timestamp as ts
    } IN TRANSACTIONS OF 1000 ROWS
    RETURN txid_n, address, txid, value, ts
    """

    result = session.run(query_through_entity, addresses=list(addresses)).values()
    rows = []
    for row in result:
        if get_average_price(row[4]) * row[3] < MIN_AMOUNT:
            continue
        rows.append((row[0], row[1], row[2]))
    return rows


def track_output(driver, context: Context):
    global MIN_AMOUNT
    MIN_AMOUNT=context.min_amount
    start_time = time.time()

    logger.info(f"Start walk. Run id: {RUN_ID}")
    # with driver.session() as session:
    summary = Summary()
    last_save = time.time()
    previous_transactions = context.previous_transactions
    output_transactions = []
    try:
        for depth in range(context.curr_depth,context.max_depth):
            added_through_entity = 0
            if not context.ignore_entities and not context.resumed:
                with driver.session() as session:
                    addresses = {l.data["address"] for l in context.input_walker.leaves() if l.data is not None and l.data.get("end") != True}
                    if addresses:
                        logger.info(f"Expanding entities through 'input' entities: {len(addresses)} addresses")
                        logger.debug(f"Getting outputs through entity for {addresses}")
                        address_to_outputs = get_inputs_through_entity(session, list(addresses))
                        for address, outputs in address_to_outputs.items():
                            for o in outputs:
                                added_through_entity += 1
                                context.input_walker.add_entity_connection(o, address)

                    addresses = context.output_walker.get_latest_addresses()
                    if addresses:
                        logger.info(f"Expanding entities through 'output' entities: {len(addresses)} addresses")
                        logger.debug(f"Getting outputs through entity for {addresses}")
                        rows = get_outputs_through_entity(session, list(addresses))
                        context.output_walker.add_entity_connection(rows)


            if context.resumed:
                transactions = list(set(context.transactions).difference(context.done_transactions))
                output_transactions = list(set(context.output_transactions).difference(context.done_transactions))
            else:
                context.input_walker.expand_transactions()

                logger.info("Preparing next transactions")
                transactions = list(set(context.input_walker.get_next_input()))
                output_transactions = list(set(context.output_walker.get_next_output()))

                if context.log_transactions:
                    transaction_logger.info(f"Transactions expanded input direction: {transactions}")
                    transaction_logger.info(f"Transactions expanded output direction: {output_transactions}")


                logger.info(f"Total transactions expanded up to now: {summary.expanded_transactions} ({summary.expanded_transactions / (time.time()-start_time):.2f} tx/s). "
                            f"Expanding {len(transactions)} input direction + {len(output_transactions)} output direction transactions. Depth: {depth}")

                logger.debug(f"TX in common: {len(previous_transactions.intersection(set(transactions)))}")
                previous_transactions = set(transactions)

            # All the resume setup should be done by now, so we're effectively back into "normal" processing
            context.resumed = False

            kill_queue = Queue()
            receive_queue = Queue()
            input_thread = Thread(target=parallel_execution, args=(driver, input_thread_function, kill_queue, transactions, receive_queue))
            input_thread.start()

            output_thread = Thread(target=parallel_execution, args=(driver, output_thread_function, kill_queue, output_transactions, receive_queue))
            output_thread.start()

            done_transactions = set()
            broken = 0
            while True:
                x = receive_queue.get()

                if x is None:
                    broken += 1
                    if broken >= 2:
                        # this only happens when the thread as finished processing all transactions and it's the only
                        # way out of the while True loop (except for successfully finding a complete path)
                        # It has to break twice because it has to break for the output and input thread
                        break
                    continue

                txid, is_input, rows = x
                if is_input:
                    context.input_walker.process_input(txid, rows, context.end_addresses)
                else:
                    context.output_walker.process_output(txid, rows)

                if context.log_transactions:
                    transaction_logger.info(f"Explored {txid}")

                done_transactions.add(txid)
                summary.expanded_transactions += 1

                intersection = context.input_walker.txids.intersection(context.output_walker.txids)
                if intersection:
                    intersect_txid = list(intersection)[0]
                    logger.info("Found path")
                    input_path = context.input_walker.get_path(intersect_txid)
                    logger.info(input_path)
                    output_path = context.output_walker.get_path(intersect_txid)
                    logger.info(output_path)
                    logger.info(path_to_query(input_path, output_path))
                    exit(0)
                # save progress every 6 hours
                if time.time() - last_save > 60*60*6:
                    logger.info("Saving progress")
                    context.resumed = True

                    # we need to save the current state to avoid re-running all the transactions at current depth
                    context.previous_transactions = previous_transactions
                    context.transactions = transactions
                    context.output_transactions = output_transactions
                    context.done_transactions = done_transactions
                    context.curr_depth = depth

                    with open(f"progress_{RUN_ID}.pickle", "wb+") as f:
                        pickle.dump(context, f)
                    context.resumed = False
                    last_save = time.time()

            context.input_walker.empty_garbage()
            summary.depth = depth
            context.curr_depth = depth
    finally:
        logger.info(f"Exited. Expanded a total of {summary.expanded_transactions} transactions up to depth {context.curr_depth}")


def resume_tracking(driver, path: str):
    global RUN_ID
    with open(pathlib.Path(path).resolve(), "rb") as f:
        ctx: Context = pickle.load(f)

    RUN_ID = path.split("_")[1].split(".")[0].strip()
    setup_logging()
    logger.info(f"Resuming from {RUN_ID}:\nMin amount: {ctx.min_amount}\nDepth: {ctx.curr_depth}->{ctx.max_depth}\n"
                f"Log transactions: {ctx.log_transactions}\nIgnore entities: {ctx.ignore_entities}")
    track_output(driver, ctx)

def setup_track_output(driver, start, end, min_amount, max_depth, ignore_entities, log_transactions):
    global MIN_AMOUNT
    MIN_AMOUNT = min_amount
    setup_logging()
    logger.info(f"Finding bitcoin trail {end}->{start}. Minimum transaction amount: {min_amount}$")
    if ignore_entities:
        logger.info("Ignoring paths through entities")

    address_to_outputs = {}
    input_walker = InputWalker()
    output_walker = OutputWalker()
    with driver.session() as session:
        logger.info(f"Getting addresses of entity {start}")
        try:
            int(start)
            name = "entity_id"
        except:
            name = "name"

        result = session.run("""
                MATCH (e:Entity {%s: $entityName})--(a:Address)--(o:Output)
                WITH a.address as address, collect(o.txid_n) as o
                RETURN address, o
            """ % name, entityName=start).values()

        for address, outputs in result:
            address_to_outputs[address] = []
            for o in outputs:
                address_to_outputs[address].append(o)
                input_walker.add_root(o)
        logger.info(f"Addresses found: {address_to_outputs.keys()}")
        logger.info(f"Getting addresses of entity {end}")
        try:
            int(end)
            name = "entity_id"
        except:
            name = "name"

        result = session.run("""
            MATCH (e:Entity {%s: $entityName})--(a:Address)--(o:Output)-[:INPUT]->(t:Transaction)<-[:CONTAINS]-(b:Block)
                WITH a.address as address, collect([o.txid_n, o.value, t.txid, b.timestamp]) as tuples
                RETURN address, tuples
            """ % name, entityName=end).values()

        end_addresses = set([])
        for address, tuples in result:
            end_addresses.add(address)
            address_to_outputs[address] = []
            for txid_n, value, txid, ts in tuples:
                if get_average_price(ts) * value < MIN_AMOUNT:
                    continue
                address_to_outputs[address].append(txid_n)
                output_walker.process_output('root', [[txid_n, address, txid]])

        logger.info(f"Addresses found: {list(end_addresses)}")

    previous_transactions = set([])
    ctx = Context(input_walker=input_walker, transactions=[], previous_transactions=set([]), log_transactions=log_transactions,
                  address_to_outputs=address_to_outputs, end_addresses=end_addresses, curr_depth=0, max_depth=max_depth,
                  ignore_entities=ignore_entities, resumed=False, done_transactions=set([]), end=end, min_amount=min_amount,
                  start=start, output_walker=output_walker, output_transactions=[])

    setup_logging()
    track_output(driver, ctx)

def get_average_price(timestamp):
    data = {
        "Aug 21": 47_130.4, "Jul 21": 41_553.7, "Jun 21": 35_026.9, "May 21": 37_298.6,
        "Apr 21": 57_720.3, "Mar 21": 58_763.7, "Feb 21": 45_164.0, "Jan 21": 33_108.1,
        "Dec 20": 28_949.4, "Nov 20": 19_698.1, "Oct 20": 13_797.3, "Sep 20": 10_776.1,
        "Aug 20": 11_644.2, "Jul 20": 11_333.4, "Jun 20": 9_135.4, "May 20": 9_454.8,
        "Apr 20": 8_629.0, "Mar 20": 6_412.5, "Feb 20": 8_543.7, "Jan 20": 9_349.1,
        "Dec 19": 7_196.4, "Nov 19": 7_546.6, "Oct 19": 9_152.6, "Sep 19": 8_284.3,
        "Aug 19": 9_594.4, "Jul 19": 10_082.0, "Jun 19": 10_818.6, "May 19": 8_558.3,
        "Apr 19": 5_320.8, "Mar 19": 4_102.3, "Feb 19": 3_816.6, "Jan 19": 3_437.2,
        "Dec 18": 3_709.4, "Nov 18": 4_039.7, "Oct 18": 6_365.9, "Sep 18": 6_635.2,
    }

    given_date = datetime.fromtimestamp(timestamp)
    date_format = "%b %y"
    closest_date = min(data.keys(), key=lambda x: abs(given_date - datetime.strptime(x, date_format)))
    return data[closest_date]

parser = argparse.ArgumentParser(
    description="This binary is used to find the shortest direct bitcoin trail from one entity to another. The entity "
                "Can be provided both as entity_id (NOT the neo4j hidden id, the value entity.id), or as a name. "
                "By direct trail, we mean it only follows the following two types of trail:"
                "(receiver)<-[:USES]<-(:Output)<-[:INPUT]<-..(:Transaction)..<-[:OUTPUT]-(:Output)-[:USES]->(owner)"
                "Additionally, it goes through entities, so for each Output, it looks at the associated address and the entity,"
                "and if there's one, it also follows all the input to the entity addresses\n"
                "The parameter --min-amount-dollars can further be used to filter out Outputs that have a value"
                "smaller than the min amount. It uses the lower bound of the BTC value for the month of the transaction\n\n"
                "Keep in mind it knows which to call based on whether the id can be converted to an integer, so if you provide "
                "a name which is actually a number, it will try to get the entity with that entity_id instead of name."
)
parser.add_argument('receiver', help="Entity name or id receiving the bitcoins", nargs="?")
parser.add_argument('owner', help="Entity name or id sending the bitcoins", nargs="?")
parser.add_argument('--resume-path', help='Progress file from which to resume', required=False)
parser.add_argument('--host', default="0.0.0.0", help='Neo4j host')
parser.add_argument('--port', default=7687, help='Neo4j port', type=int)
parser.add_argument('-u', '--username', required=True, help='Neo4j user')
parser.add_argument('-p', '--password', required=True, help='Neo4j password')
parser.add_argument('-d', '--max-depth', required=False, help='Max depth to expand (in transactions)', default=100, type=int)
parser.add_argument('-m', '--min-amount-dollars', required=False, default=1000, type=int,
                    help="Skip expanding outputs if the associated value is lower than this (in dollars).")
parser.add_argument("--ignore-entities", action="store_true",
                    help="Do not go through entities along the way.")
parser.add_argument("--log-transactions", action="store_true",
                    help="Log all transactions explored")


if __name__ == "__main__":
    args = parser.parse_args()
    if (args.receiver is None or args.owner is None) and args.resume_path is None:
        print("You must either pass receiver and owner, or a resume path")
        exit(1)

    driver = GraphDatabase.driver(f"bolt://{args.host}:{args.port}",
                                  auth=(args.username, args.password),
                                  connection_timeout=3600)

    if args.resume_path:
        resume_tracking(driver, args.resume_path)
    else:
        setup_track_output(driver, args.receiver, args.owner, args.min_amount_dollars, args.max_depth, args.ignore_entities,
                       args.log_transactions)

